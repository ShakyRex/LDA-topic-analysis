{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Actions required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Mallet_path \n",
    "download file http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip <br>\n",
    "unzip it <br>\n",
    "save the path to the unzipped file and paste it here below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/Documents/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Data import\n",
    "import data from csv with the text to analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Documents/Data/dataset.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename column containing the text to analyse 'content', see example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"column\": \"content\"})\n",
    "df = df[df['content'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Define language of the content to analyse and download the requirements\n",
    "Follow this link, https://spacy.io/usage/models go to quickstart,<br>\n",
    "select the language and loading style (suggested 'import as module'). <br>\n",
    "\n",
    "Execute the line in the terminal and paste and execute the import and nlp line in the cell underneath here. See the example below for German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import de_core_news_sm\n",
    "nlp = de_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name the variable 'language' for the stop words https://pypi.org/project/stop-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'german'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Output data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the location and name of the analysis output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/Documents/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'topic_analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Missing imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely some libraries are missing from your machine, you can install them directly from here using the command: !pip install <library name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip download de_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giacomofederle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(language)\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideal_number_topics(id2word, corpus, texts, limit, start=2, step=3):\n",
    "    dicts = {}\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word = id2word)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary =id2word,\n",
    "                                        coherence='c_v').get_coherence()\n",
    "        dicts[num_topics] = coherencemodel\n",
    "    max_coherence = max(dicts.values())\n",
    "    ideal_number_of_topics = [k for k, v in dicts.items() if v == max_coherence]\n",
    "    return ideal_number_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return (sent_topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.content.values.tolist()\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = ideal_number_topics(id2word = id2word,\n",
    "                               corpus=corpus,\n",
    "                               texts=data_lemmatized, start=2,\n",
    "                               limit=15,\n",
    "                               step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = int(n_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_ideal = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = n_topics,\n",
    "                                              random_state = 100,\n",
    "                                              update_every = 1,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = 'auto',\n",
    "                                              per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel = lda_model_ideal,\n",
    "                                                  corpus = corpus,\n",
    "                                                  texts = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No',\n",
    "                                 'Dominant_Topic',\n",
    "                                 'Topic_Perc_Contrib', \n",
    "                                 'Keywords', \n",
    "                                 'Most_representative_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic',\n",
    "                                              'Topic_Keywords']]\n",
    "df_dominant_topics = pd.concat([topic_num_keywords,\n",
    "                                topic_counts, \n",
    "                                topic_contribution],\n",
    "                                axis=1)\n",
    "df_dominant_topics.columns = ['Dominant_Topic',\n",
    "                              'Topic_Keywords',\n",
    "                              'Num_Documents', \n",
    "                              'Perc_Documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], \n",
    "                                             ascending=[0]).head(1)], \n",
    "                                             axis=0)  \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \n",
    "                                       \"Topic_Perc_Contrib\", \n",
    "                                       \"Keywords\", \n",
    "                                       \"Most_representative_comment\"]\n",
    "# what is topic perc contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet['Topic_Num'] = sent_topics_sorteddf_mallet['Topic_Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet['Topic_Num'] = sent_topics_sorteddf_mallet['Topic_Num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = pd.DataFrame(df_topic_sents_keywords['Dominant_Topic'].value_counts())\n",
    "topic_counts['Topic_Num'] = topic_counts.index\n",
    "topic_counts['Topic_Num'] = topic_counts['Topic_Num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(sent_topics_sorteddf_mallet, \n",
    "               topic_counts,\n",
    "              on = 'Topic_Num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns={\"Dominant_Topic\": \"Count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Topic_Num'] = df2.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(output_path+output_name+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
